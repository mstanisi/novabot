pip install numpy


import numpy as np
print(np.__version__)  # Should print the version (e.g., '1.23.5')


path = "/Users/markos98/novabot/data/"


from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import Dataset
import torch
import numpy as np

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token  # Set padding token
model = GPT2LMHeadModel.from_pretrained("distilgpt2")

# Load and preprocess Naked Lunch text
with open(path + "naked_lunch.txt", "r") as f:
    text = f.read()

# Tokenize with 128-token chunks (adjust for MacBook Air memory)
inputs = tokenizer(
    text,
    truncation=True,
    max_length=128,
    stride=64,
    return_overflowing_tokens=True,
    return_tensors="pt",
)

# Add labels (required for training)
inputs["labels"] = inputs["input_ids"].clone()

# Convert to Hugging Face Dataset
dataset = Dataset.from_dict({
    "input_ids": inputs["input_ids"],
    "attention_mask": inputs["attention_mask"],
    "labels": inputs["labels"],
})

# Training arguments (optimized for MacBook Air)
training_args = TrainingArguments(
    output_dir="./results",  # This will create the folder
    per_device_train_batch_size=1,  # Reduce if OOM errors occur
    num_train_epochs=3,            # More epochs = better style
    logging_dir="./logs",
    save_steps=500,
)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
)

trainer.train()
print("Training complete! Model saved to ./results")


from collections import defaultdict
import random
import re

# Load vocabulary
valid_words = set(line.strip().lower() for line in open(path + "hoard.txt"))

# Build transitions from Naked Lunch
transitions = defaultdict(list)
with open(path + "naked_lunch.txt", "r") as f:
    text = f.read().lower()
    words = re.findall(r"\w+", text)
    for i in range(len(words) - 1):
        if words[i] in valid_words and words[i+1] in valid_words:
            transitions[words[i]].append(words[i+1])
        

# Generate text
def generate_text(start_word, length=20):
    current_word = start_word.lower()
    output = [current_word]
    for _ in range(length - 1):
        next_words = transitions.get(current_word, [])
        if not next_words:
            next_words = list(valid_words)
        current_word = random.choice(next_words)
        output.append(current_word)
    return " ".join(output)


markov_seed = generate_text("junk", length=30)
print(f"Markov Seed: {markov_seed}")


from transformers import pipeline

# Load your fine-tuned model
generator = pipeline("text-generation", model="./results", tokenizer="distilgpt2")

# Constrain output to hoard.txt words
bad_words_ids = [[id] for id in range(tokenizer.vocab_size) 
                if id not in allowed_token_ids]  # From earlier

# Generate LLM output
llm_output = generator(
    markov_seed,
    max_length=50,
    do_sample=True,
    top_p=0.9,
    repetition_penalty=1.5,
    bad_words_ids=bad_words_ids,
    num_return_sequences=1
)[0]["generated_text"]

print(f"LLM Output: {llm_output}")



